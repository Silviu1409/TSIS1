# Import other libraries
import pickle
import matplotlib.pyplot as plt
import seaborn as sbs
import litstudy
import glob
import os
import io
import base64
import pandas as pd
import matplotlib.backends.backend_agg as agg
from pandas.plotting import table
import shutil
import logging
from pyvis.network import Network


# Ensure the 'static/graphs' directory exists
if not os.path.exists('static/graphs'):
    os.makedirs('static/graphs')

# Set plot and logging configuration
plt.rcParams['figure.figsize'] = (10, 6)
sbs.set('paper')
# logging.basicConfig(level=logging.DEBUG)
logging.getLogger().setLevel(logging.CRITICAL)

net = Network(notebook=True, cdn_resources='remote')


def load_docs_from_pickle(pickle_file_path):
    """Loads docs from the pickle file if it exists."""
    
    if os.path.exists(pickle_file_path):
        with open(pickle_file_path, 'rb') as f:
            docs = pickle.load(f)
            print(len(docs), 'papers loaded from pickle')
    else:
        docs = set()  # If the pickle file doesn't exist, start with an empty set
    
    return docs


def save_docs_to_pickle(docs, pickle_file_path):
    """Saves docs_remaining to the pickle file."""
    
    with open(pickle_file_path, 'wb') as f:
        pickle.dump(docs, f)
        print(len(docs), 'papers saved to pickle')


def load_and_process_files(files, pickle_file_path='data/docs_remaining.pkl'):
    """Loads and merges documents from CSV files, excludes documents based on RIS files and saves the updated docs_remaining to a pickle file."""

    # Load existing docs_remaining from pickle, if available
    docs_remaining = load_docs_from_pickle(pickle_file_path)

    docs_csv = docs_remaining  # Start with docs_remaining, and merge with new CSV data
    docs_exclude = None

    # Iterate through the uploaded files
    for file in files:
        
        file_path = os.path.join('data', file.filename)
        file.save(file_path)  # Save the uploaded file

        if file.filename.endswith('.csv'):
            # If it's a CSV file, load and add to docs_csv
            doc = litstudy.load_csv(file_path)
            docs_csv = docs_csv | doc
        elif file.filename.endswith('.ris'):
            # If it's a RIS file, load and add to docs_exclude
            doc = litstudy.load_ris_file(file_path)
            docs_exclude = docs_exclude | doc

    # Exclude documents from the RIS files
    docs_remaining = docs_csv - docs_exclude

    # Save the updated docs_remaining back to the pickle file
    save_docs_to_pickle(docs_remaining, pickle_file_path)

    # Refine docs using CrossRef
    docs_crossref = refine_and_save_crossref(docs_remaining)

    # Generate new histograms and save them in /static/graphs
    plot_histograms(docs_remaining)

    # Generate the co-citation and coupling networks and save them in /static/graphs
    plot_cocitation_network(docs_crossref)
    plot_coupling_network(docs_crossref)

    # Generate stats
    generate_stats(docs_remaining)


def refine_and_save_crossref(docs_remaining, pickle_file_path='data/docs_crossref.pkl'):
    """Refines docs_remaining using CrossRef, separates found and not-found papers and saves the refined docs_crossref to a pickle file."""
    
    docs_crossref, docs_notfound = litstudy.refine_crossref(docs_remaining)

    print()
    print(len(docs_crossref), 'papers found on CrossRef')
    print(len(docs_notfound), 'papers were not found and were discarded')

    # Save docs_crossref to a pickle file
    save_docs_to_pickle(docs_crossref, pickle_file_path)

    return docs_crossref


def plot_histograms(docs):
    """Generate and save the histogram plots."""

    try:
        # Plot Year Histogram
        litstudy.plot_year_histogram(docs, vertical=True)
        plt.savefig('static/graphs/year_histogram.png')
        plt.close()

        # Plot Affiliation Histogram
        litstudy.plot_affiliation_histogram(docs, limit=15)
        plt.savefig('static/graphs/affiliation_histogram.png')
        plt.close()

        # Plot Author Histogram
        litstudy.plot_author_histogram(docs)
        plt.savefig('static/graphs/author_histogram.png')
        plt.close()

        # Plot Number of Authors Histogram
        litstudy.plot_number_authors_histogram(docs)
        plt.savefig('static/graphs/number_authors_histogram.png')
        plt.close()

        logging.info("Histograms saved successfully.")
    except Exception as e:
        logging.error(f"Error generating histograms: {e}")


def plot_cocitation_network(docs_crossref):
    """Generate and save the cocitation network graph to the desired location."""
    try:
        # Generate the network graph, which outputs 'citation.html'
        litstudy.plot_cocitation_network(docs_crossref)

        # Source file generated by litstudy
        source_file = 'citation.html'

        # Destination path
        output_path = 'static/graphs/cocitation_network.html'

        # Ensure the source file exists
        if not os.path.exists(source_file):
            raise FileNotFoundError(f"Expected file {source_file} not found.")

        # Move and rename the file to the desired location
        shutil.move(source_file, output_path)
        logging.info(f"Co-citation network successfully moved to {output_path}.")
    except Exception as e:
        logging.error(f"Error generating or moving co-citation network: {e}")


def plot_coupling_network(docs_crossref, max_edges=5000):
    """Generate and save the coupling network graph to the desired location."""
    
    try:
        # Generate the network graph, which outputs 'citation.html'
        litstudy.plot_coupling_network(docs_crossref, max_edges=max_edges)

        # Source file generated by litstudy
        source_file = 'citation.html'

        # Destination path
        output_path = 'static/graphs/coupling_network.html'

        # Ensure the source file exists
        if not os.path.exists(source_file):
            raise FileNotFoundError(f"Expected file {source_file} not found.")

        # Move and rename the file to the desired location
        shutil.move(source_file, output_path)
        logging.info(f"Coupling network successfully moved to {output_path}.")
    except Exception as e:
        logging.error(f"Error generating or moving coupling network: {e}")


def save_table_as_png(dataframe, path, figsize=(8, 6), scale_factor=1.2):
    fig, ax = plt.subplots(figsize=figsize)  # Set the figure size
    ax.axis('off')  # Hide the axes

    # Create the table
    tbl = table(ax, dataframe, loc='center', colWidths=[0.3] * len(dataframe.columns))

    # Set font size and scale
    tbl.auto_set_font_size(False)
    tbl.set_fontsize(10)
    tbl.scale(scale_factor, scale_factor)  # Adjust the scale for better fit

    # Adjust layout to center the table
    plt.tight_layout(pad=2.0)  # Add padding to center the table

    # Save the table as PNG
    plt.savefig(path, bbox_inches='tight', dpi=300)
    plt.close()


def generate_stats(docs, num_topics=15, ngram_threshold=0.8, save_dir="static/graphs"):
    # Build the corpus
    corpus = litstudy.build_corpus(docs, ngram_threshold=ngram_threshold)
    
    # 1. Save word distribution plot (graph)
    word_dist_path = os.path.join(save_dir, "word_distribution.png")
    litstudy.plot_word_distribution(corpus, limit=50, title="Top words", vertical=True, label_rotation=45)
    plt.savefig(word_dist_path, bbox_inches='tight', dpi=300)
    plt.close()

    # 2. Save word distribution table as PNG
    word_dist_table_path = os.path.join(save_dir, "word_distribution_table.png")
    word_dist_table = litstudy.compute_word_distribution(corpus).filter(like='_', axis=0).sort_index().sort_values(by='count', ascending=False)
    save_table_as_png(word_dist_table, word_dist_table_path, figsize=(12, 6), scale_factor=1.5)

    # Train the topic model
    topic_model = litstudy.train_nmf_model(corpus, num_topics=num_topics, max_iter=100)
    
    # 3. Save topic modeling table as PNG
    topic_table_path = os.path.join(save_dir, "topic_modeling_table.png")
    topics_data = []
    for i in range(num_topics):
        best_tokens = topic_model.best_tokens_for_topic(i)
        # Limit token length to avoid overlap
        truncated_tokens = ", ".join(best_tokens[:5]) + ("..." if len(best_tokens) > 5 else "")
        topics_data.append({"Top Tokens": truncated_tokens})

    # Create the DataFrame and set the index to start from 1
    topic_table = pd.DataFrame(topics_data, index=range(1, len(topics_data) + 1))
    save_table_as_png(topic_table, topic_table_path, figsize=(14, 6), scale_factor=1.5)

    # 4. Save topic clouds plot (graph)
    topic_cloud_path = os.path.join(save_dir, "topic_cloud.png")
    plt.figure(figsize=(15, 5))
    litstudy.plot_topic_clouds(topic_model, ncols=5)
    plt.savefig(topic_cloud_path, bbox_inches='tight', dpi=300)
    plt.close()


def filter_by_title(query):
    """Filter documents based on a keyword."""
    
    docs = load_docs_from_pickle('data/docs_remaining.pkl')
    query_lower = query.lower()

    filtered_docs = [d for d in docs 
        if query_lower in d.title.lower() or any(query_lower in keyword.lower() for sublist in (d.keywords or []) for keyword in sublist)
    ]

    print(f"\nSearch Results for '{query}':")
    if not filtered_docs:
        print("No documents found.")
        return

    # Limit the output to top 10 results
    for i, doc in enumerate(filtered_docs[:10], 1):
        # print(f"{i}. Document Details:")
        # for field in dir(doc):
        #     # Filter out private and built-in attributes (starting with '_')
        #     if not field.startswith('_'):
        #         value = getattr(doc, field)
        #         try:
        #             # Format the output for lists
        #             if isinstance(value, list):
        #                 value = ', '.join(str(item) for item in value) if value else 'N/A'
        #             # Truncate long strings
        #             elif isinstance(value, str) and len(value) > 200:
        #                 value = f"{value[:200]}..."
        #             # Handle other types (convert to string)
        #             else:
        #                 value = str(value) if value is not None else 'N/A'
        #         except Exception as e:
        #             value = f"Error converting field: {e}"
        #         print(f"   {field.capitalize()}: {value}")
        print(f"{i}. Document keywords: {', '.join(str(item) for item in doc.keywords) if doc.keywords and len(doc.keywords) > 0 else 'N/A'}")
        print("\n" + "="*50 + "\n")

    # If there are more results, indicate that not all were shown
    if len(filtered_docs) > 10:
        print(f"...and {len(filtered_docs) - 10} more results not shown.")
    
    return filtered_docs
